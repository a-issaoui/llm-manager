{
  "DeepSeek-R1-Distill-Llama-3B.Q5_K_M.gguf": {
    "specs": {
      "architecture": "llama",
      "quantization": "Q5_K_M",
      "size_label": "3B",
      "parameters_b": 3.0,
      "layer_count": 28,
      "context_window": 131072,
      "file_size_mb": 2214,
      "hidden_size": 3072,
      "head_count": 32,
      "head_count_kv": 32,
      "feed_forward_size": 8192,
      "vocab_size": 128256,
      "expert_count": null,
      "active_expert_count": null,
      "rope_freq_base": 500000.0,
      "rope_freq_scale": null,
      "rope_scaling_type": null,
      "rope_scaling_factor": null,
      "attention_layer_norm_rms_epsilon": 9.999999747378752e-06,
      "attention_type": null,
      "gqa_ratio": 1,
      "moe_shared_expert_count": null,
      "moe_router_type": null,
      "moe_shared_expert_intermediate_size": null,
      "sliding_window": null,
      "temporal_patch_size": null,
      "spatial_patch_size": null,
      "tokenizer": {
        "bos_token_id": 128000,
        "eos_token_id": 128009,
        "padding_token_id": 128009,
        "model": "gpt2",
        "pre": "llama-bpe"
      },
      "audio": null,
      "custom_chat_template": false,
      "context_test": {
        "max_context": 32768,
        "recommended_context": 26214,
        "buffer_tokens": 6554,
        "buffer_percent": 20,
        "tested": false,
        "verified_stable": false,
        "error": null,
        "test_config": {},
        "timestamp": "",
        "confidence": 1.0
      },
      "optimized_kv_quant": false,
      "file_hash": "5e4d65e2443771b2"
    },
    "capabilities": {
      "chat": true,
      "embed": false,
      "vision": false,
      "audio_in": false,
      "reasoning": true,
      "tools": false
    },
    "prompt": {
      "template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"
    },
    "mmproj": null,
    "path": "models/DeepSeek-R1-Distill-Llama-3B.Q5_K_M.gguf"
  },
  "Nanbeige_Nanbeige4-3B-Thinking-2511-Q4_K_M.gguf": {
    "specs": {
      "architecture": "llama",
      "quantization": "Q4_K_M",
      "size_label": "3B",
      "parameters_b": 3.0,
      "layer_count": 32,
      "context_window": 65536,
      "file_size_mb": 2329,
      "hidden_size": 2560,
      "head_count": 32,
      "head_count_kv": 32,
      "feed_forward_size": 10496,
      "vocab_size": 166144,
      "expert_count": null,
      "active_expert_count": null,
      "rope_freq_base": 5000000.0,
      "rope_freq_scale": null,
      "rope_scaling_type": null,
      "rope_scaling_factor": null,
      "attention_layer_norm_rms_epsilon": 9.999999747378752e-06,
      "attention_type": null,
      "gqa_ratio": 1,
      "moe_shared_expert_count": null,
      "moe_router_type": null,
      "moe_shared_expert_intermediate_size": null,
      "sliding_window": null,
      "temporal_patch_size": null,
      "spatial_patch_size": null,
      "tokenizer": {
        "bos_token_id": 166100,
        "eos_token_id": 166101,
        "padding_token_id": 0,
        "model": "llama",
        "pre": "default"
      },
      "audio": null,
      "custom_chat_template": false,
      "context_test": {
        "max_context": 32768,
        "recommended_context": 26214,
        "buffer_tokens": 6554,
        "buffer_percent": 20,
        "tested": false,
        "verified_stable": false,
        "error": null,
        "test_config": {},
        "timestamp": "",
        "confidence": 1.0
      },
      "optimized_kv_quant": false,
      "file_hash": "e5451406dc0ba0b9"
    },
    "capabilities": {
      "chat": true,
      "embed": false,
      "vision": false,
      "audio_in": false,
      "reasoning": true,
      "tools": true
    },
    "prompt": {
      "template": "\n        {%- if tools %}\n            {{- '<|im_start|>system\n' }}\n            {%- if messages[0].role == 'system' %}\n                {{- messages[0].content + '\n\n' }}\n            {%- else %} \n                {{- '你是一位工具函数调用专家，你会得到一个问题和一组可能的工具函数。根据问题，你需要进行一个或多个函数/工具调用以实现目的，请尽量尝试探索通过工具解决问题。\n如果没有一个函数可以使用，请直接使用自然语言回复用户。\n如果给定的问题缺少函数所需的参数，请使用自然语言进行提问，向用户询问必要信息。\n如果调用结果已经足够回答用户问题，请对历史结果进行总结，使用自然语言回复用户。' }} \n            {%- endif %}\n            {{- \"# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\" }}\n            {%- for tool in tools %}\n                {{- \"\n\" }}\n                {{- tool | tojson }}\n            {%- endfor %}\n            {{- \"\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\n</tool_call><|im_end|>\n\" }}\n        {%- else %}\n            {%- if messages[0].role == 'system' %}\n                {{- '<|im_start|>system\n' + messages[0].content + '<|im_end|>\n' }}\n            {%- else %} \n                {{- '<|im_start|>system\n你是南北阁，一款由BOSS直聘自主研发并训练的专业大语言模型。<|im_end|>\n' }} \n            {%- endif %}\n        {%- endif %}\n        {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n        {%- for message in messages[::-1] %}\n            {%- set index = (messages|length - 1) - loop.index0 %}\n            {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n                {%- set ns.multi_step_tool = false %}\n                {%- set ns.last_query_index = index %}\n            {%- endif %}\n        {%- endfor %}\n        {%- for message in messages %}\n            {%- if message.content is string %}\n                {%- set content = message.content %}\n            {%- else %}\n                {%- set content = '' %}\n            {%- endif %}\n            {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n                {{- '<|im_start|>' + message.role + '\n' + content + '<|im_end|>' + '\n' }}\n            {%- elif message.role == \"assistant\" %}\n                {%- set reasoning_content = '' %}\n                {%- if message.reasoning_content is string %}\n                    {%- set reasoning_content = message.reasoning_content %}\n                {%- else %}\n                    {%- if '</think>' in content %}\n                        {%- set reasoning_content = content.split('</think>')[0].rstrip('\n').split('<think>')[-1].lstrip('\n') %}\n                        {%- set content = content.split('</think>')[-1].lstrip('\n') %}\n                    {%- endif %}\n                {%- endif %}\n                {%- if loop.index0 > ns.last_query_index %}\n                    {%- if loop.last or (not loop.last and reasoning_content) %}\n                        {{- '<|im_start|>' + message.role + '\n<think>\n' + reasoning_content.strip('\n') + '\n</think>\n\n' + content.lstrip('\n') }}\n                    {%- else %}\n                        {{- '<|im_start|>' + message.role + '\n' + content }}\n                    {%- endif %}\n                {%- else %}\n                    {{- '<|im_start|>' + message.role + '\n' + content }}\n                {%- endif %}\n                {%- if message.tool_calls %}\n                    {%- for tool_call in message.tool_calls %}\n                        {%- if (loop.first and content) or (not loop.first) %}\n                            {{- '\n' }}\n                        {%- endif %}\n                        {%- if tool_call.function %}\n                            {%- set tool_call = tool_call.function %}\n                        {%- endif %}\n                        {{- '<tool_call>\n{\"name\": \"' }}\n                        {{- tool_call.name }}\n                        {{- '\", \"arguments\": ' }}\n                        {%- if tool_call.arguments is string %}\n                            {{- tool_call.arguments }}\n                        {%- else %}\n                            {{- tool_call.arguments | tojson }}\n                        {%- endif %}\n                        {{- '}\n</tool_call>' }}\n                    {%- endfor %}\n                {%- endif %}\n                {{- '<|im_end|>\n' }}\n            {%- elif message.role == \"tool\" %}\n                {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n                    {{- '<|im_start|>user' }}\n                {%- endif %}\n                {{- '\n<tool_response>\n' }}\n                {{- content }}\n                {{- '\n</tool_response>' }}\n                {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n                    {{- '<|im_end|>\n' }}\n                {%- endif %}\n            {%- endif %}\n        {%- endfor %}\n        {%- if add_generation_prompt %}\n            {{- '<|im_start|>assistant\n' }}\n        {%- endif %}\n"
    },
    "mmproj": null,
    "path": "models/Nanbeige_Nanbeige4-3B-Thinking-2511-Q4_K_M.gguf"
  },
  "Qwen2.5-3b-instruct-q4_k_m.gguf": {
    "specs": {
      "architecture": "qwen2",
      "quantization": "Q4_K_M",
      "size_label": "3B",
      "parameters_b": 3.0,
      "layer_count": 36,
      "context_window": 32768,
      "file_size_mb": 1840,
      "hidden_size": 2048,
      "head_count": 32,
      "head_count_kv": 32,
      "feed_forward_size": 11008,
      "vocab_size": 151936,
      "expert_count": null,
      "active_expert_count": null,
      "rope_freq_base": 1000000.0,
      "rope_freq_scale": null,
      "rope_scaling_type": null,
      "rope_scaling_factor": null,
      "attention_layer_norm_rms_epsilon": 9.999999974752427e-07,
      "attention_type": null,
      "gqa_ratio": 1,
      "moe_shared_expert_count": null,
      "moe_router_type": null,
      "moe_shared_expert_intermediate_size": null,
      "sliding_window": null,
      "temporal_patch_size": null,
      "spatial_patch_size": null,
      "tokenizer": {
        "bos_token_id": 151643,
        "eos_token_id": 151645,
        "padding_token_id": 151643,
        "model": "gpt2",
        "pre": "qwen2"
      },
      "audio": null,
      "custom_chat_template": false,
      "context_test": {
        "max_context": 16384,
        "recommended_context": 13107,
        "buffer_tokens": 3277,
        "buffer_percent": 20,
        "tested": false,
        "verified_stable": false,
        "error": null,
        "test_config": {},
        "timestamp": "",
        "confidence": 1.0
      },
      "optimized_kv_quant": false,
      "file_hash": "2eeca7a05a29f3ca"
    },
    "capabilities": {
      "chat": true,
      "embed": false,
      "vision": false,
      "audio_in": false,
      "reasoning": false,
      "tools": true
    },
    "prompt": {
      "template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n"
    },
    "mmproj": null,
    "path": "models/Qwen2.5-3b-instruct-q4_k_m.gguf"
  },
  "Qwen2.5-Coder-3B-Instruct-Q4_K_M.gguf": {
    "specs": {
      "architecture": "qwen2",
      "quantization": "Q4_K_M",
      "size_label": "3B",
      "parameters_b": 3.0,
      "layer_count": 36,
      "context_window": 32768,
      "file_size_mb": 1840,
      "hidden_size": 2048,
      "head_count": 32,
      "head_count_kv": 32,
      "feed_forward_size": 11008,
      "vocab_size": 151936,
      "expert_count": null,
      "active_expert_count": null,
      "rope_freq_base": 1000000.0,
      "rope_freq_scale": null,
      "rope_scaling_type": null,
      "rope_scaling_factor": null,
      "attention_layer_norm_rms_epsilon": 9.999999974752427e-07,
      "attention_type": null,
      "gqa_ratio": 1,
      "moe_shared_expert_count": null,
      "moe_router_type": null,
      "moe_shared_expert_intermediate_size": null,
      "sliding_window": null,
      "temporal_patch_size": null,
      "spatial_patch_size": null,
      "tokenizer": {
        "bos_token_id": 151643,
        "eos_token_id": 151645,
        "padding_token_id": 151643,
        "model": "gpt2",
        "pre": "qwen2"
      },
      "audio": null,
      "custom_chat_template": false,
      "context_test": {
        "max_context": 16384,
        "recommended_context": 13107,
        "buffer_tokens": 3277,
        "buffer_percent": 20,
        "tested": false,
        "verified_stable": false,
        "error": null,
        "test_config": {},
        "timestamp": "",
        "confidence": 1.0
      },
      "optimized_kv_quant": false,
      "file_hash": "df48ee92eee8a8e7"
    },
    "capabilities": {
      "chat": true,
      "embed": false,
      "vision": false,
      "audio_in": false,
      "reasoning": false,
      "tools": true
    },
    "prompt": {
      "template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n"
    },
    "mmproj": null,
    "path": "models/Qwen2.5-Coder-3B-Instruct-Q4_K_M.gguf"
  },
  "Qwen3-Embedding-0.6B-Q8_0.gguf": {
    "specs": {
      "architecture": "qwen3",
      "quantization": "Q5_1",
      "size_label": "0.6B",
      "parameters_b": 0.6,
      "layer_count": 28,
      "context_window": 32768,
      "file_size_mb": 609,
      "hidden_size": 1024,
      "head_count": 32,
      "head_count_kv": 32,
      "feed_forward_size": 3072,
      "vocab_size": 151669,
      "expert_count": null,
      "active_expert_count": null,
      "rope_freq_base": 1000000.0,
      "rope_freq_scale": null,
      "rope_scaling_type": null,
      "rope_scaling_factor": null,
      "attention_layer_norm_rms_epsilon": 9.999999974752427e-07,
      "attention_type": null,
      "gqa_ratio": 1,
      "moe_shared_expert_count": null,
      "moe_router_type": null,
      "moe_shared_expert_intermediate_size": null,
      "sliding_window": null,
      "temporal_patch_size": null,
      "spatial_patch_size": null,
      "tokenizer": {
        "bos_token_id": 151643,
        "eos_token_id": 151643,
        "padding_token_id": 151643,
        "model": "gpt2",
        "pre": "qwen2"
      },
      "audio": null,
      "custom_chat_template": false,
      "context_test": {
        "max_context": 16384,
        "recommended_context": 13107,
        "buffer_tokens": 3277,
        "buffer_percent": 20,
        "tested": false,
        "verified_stable": false,
        "error": null,
        "test_config": {},
        "timestamp": "",
        "confidence": 1.0
      },
      "optimized_kv_quant": false,
      "file_hash": "db529b4d7801fa98"
    },
    "capabilities": {
      "chat": false,
      "embed": true,
      "vision": false,
      "audio_in": false,
      "reasoning": false,
      "tools": false
    },
    "prompt": {
      "template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in message.content %}\n                {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '<think>\\n\\n</think>\\n\\n' }}\n    {%- endif %}\n{%- endif %}"
    },
    "mmproj": null,
    "path": "models/Qwen3-Embedding-0.6B-Q8_0.gguf"
  },
  "Reason-With-Choice-3B.Q4_K_M.gguf": {
    "specs": {
      "architecture": "llama",
      "quantization": "Q4_K_M",
      "size_label": "3B",
      "parameters_b": 3.0,
      "layer_count": 28,
      "context_window": 131072,
      "file_size_mb": 1925,
      "hidden_size": 3072,
      "head_count": 32,
      "head_count_kv": 32,
      "feed_forward_size": 8192,
      "vocab_size": 128256,
      "expert_count": null,
      "active_expert_count": null,
      "rope_freq_base": 500000.0,
      "rope_freq_scale": null,
      "rope_scaling_type": null,
      "rope_scaling_factor": null,
      "attention_layer_norm_rms_epsilon": 9.999999747378752e-06,
      "attention_type": null,
      "gqa_ratio": 1,
      "moe_shared_expert_count": null,
      "moe_router_type": null,
      "moe_shared_expert_intermediate_size": null,
      "sliding_window": null,
      "temporal_patch_size": null,
      "spatial_patch_size": null,
      "tokenizer": {
        "bos_token_id": 128000,
        "eos_token_id": 128009,
        "padding_token_id": 128004,
        "model": "gpt2",
        "pre": "llama-bpe"
      },
      "audio": null,
      "custom_chat_template": false,
      "context_test": {
        "max_context": 32768,
        "recommended_context": 26214,
        "buffer_tokens": 6554,
        "buffer_percent": 20,
        "tested": false,
        "verified_stable": false,
        "error": null,
        "test_config": {},
        "timestamp": "",
        "confidence": 1.0
      },
      "optimized_kv_quant": false,
      "file_hash": "074e33b9e23f940e"
    },
    "capabilities": {
      "chat": true,
      "embed": false,
      "vision": false,
      "audio_in": false,
      "reasoning": false,
      "tools": true
    },
    "prompt": {
      "template": "{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n    {%- else %}\n        {%- set date_string = \"26 Jul 2024\" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n        {{- '\"parameters\": ' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \"}\" }}\n        {{- \"<|eot_id|>\" }}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n"
    },
    "mmproj": null,
    "path": "models/Reason-With-Choice-3B.Q4_K_M.gguf"
  },
  "SmolLM-1.7B-Instruct-Q4_K_M.gguf": {
    "specs": {
      "architecture": "llama",
      "quantization": "Q4_K_M",
      "size_label": "1.7B",
      "parameters_b": 1.7,
      "layer_count": 24,
      "context_window": 2048,
      "file_size_mb": 1006,
      "hidden_size": 2048,
      "head_count": 32,
      "head_count_kv": 32,
      "feed_forward_size": 8192,
      "vocab_size": 49152,
      "expert_count": null,
      "active_expert_count": null,
      "rope_freq_base": 10000.0,
      "rope_freq_scale": null,
      "rope_scaling_type": null,
      "rope_scaling_factor": null,
      "attention_layer_norm_rms_epsilon": 9.999999747378752e-06,
      "attention_type": null,
      "gqa_ratio": 1,
      "moe_shared_expert_count": null,
      "moe_router_type": null,
      "moe_shared_expert_intermediate_size": null,
      "sliding_window": null,
      "temporal_patch_size": null,
      "spatial_patch_size": null,
      "tokenizer": {
        "bos_token_id": 1,
        "eos_token_id": 2,
        "padding_token_id": 16,
        "model": "gpt2",
        "pre": "smollm"
      },
      "audio": null,
      "custom_chat_template": false,
      "context_test": {
        "max_context": 1024,
        "recommended_context": 819,
        "buffer_tokens": 205,
        "buffer_percent": 20,
        "tested": false,
        "verified_stable": false,
        "error": null,
        "test_config": {},
        "timestamp": "",
        "confidence": 1.0
      },
      "optimized_kv_quant": false,
      "file_hash": "78d14e01e496db5a"
    },
    "capabilities": {
      "chat": true,
      "embed": false,
      "vision": false,
      "audio_in": false,
      "reasoning": false,
      "tools": true
    },
    "prompt": {
      "template": "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"
    },
    "mmproj": null,
    "path": "models/SmolLM-1.7B-Instruct-Q4_K_M.gguf"
  }
}